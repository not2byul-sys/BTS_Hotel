"""
ARMY Stay Hub - ë‹¤ì¤‘ í”Œë«í¼ í•œêµ­ OTA ìŠ¤í¬ë˜í¼
ì—¬ëŸ¬ í”Œë«í¼ì—ì„œ ë¶„ì‚° ìˆ˜ì§‘í•˜ì—¬ ë¦¬ìŠ¤í¬ ë¶„ì‚° + ì •ë³´ í’ë¶€í™”

ì§€ì› í”Œë«í¼:
1. Agoda - ê¸€ë¡œë²Œ OTA, ì˜ì–´ ë°ì´í„°
2. ë„¤ì´ë²„ í˜¸í…” - êµ­ë‚´ ìµœëŒ€ í¬í„¸
3. ì—¬ê¸°ì–´ë•Œ (GoodChoice) - êµ­ë‚´ ìˆ™ë°• ì•±
4. ì•¼ë†€ì (Yanolja) - êµ­ë‚´ ìˆ™ë°• ì•±
5. ì¿ íŒ¡ íŠ¸ë˜ë¸” - ì´ì»¤ë¨¸ìŠ¤ ìˆ™ë°•

ì£¼ì˜: ê°œì¸ MVPìš© ì†ŒëŸ‰ ìŠ¤í¬ë˜í•‘. ìƒì—…ì  ëŒ€ê·œëª¨ ì‚¬ìš© ê¸ˆì§€.
"""

import requests
import random
import time
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from abc import ABC, abstractmethod
from urllib.parse import urlencode, quote

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None
    print("âš ï¸ BeautifulSoup not installed. Run: pip install beautifulsoup4")


class BaseScraper(ABC):
    """ëª¨ë“  OTA ìŠ¤í¬ë˜í¼ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.name = "Base"
        self.name_kr = "ê¸°ë³¸"
        self.base_url = ""

        # ê³µí†µ í—¤ë”
        self.headers = {
            "User-Agent": self._get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        # í‚¨í…ìŠ¤/ê³ ì–‘ì‹œ ì§€ì—­ ì„¤ì •
        self.target_coords = {"lat": 37.6694, "lng": 126.7456}
        self.target_areas = ["ê³ ì–‘", "ì¼ì‚°", "í‚¨í…ìŠ¤", "Goyang", "Ilsan", "KINTEX"]

    def _get_random_user_agent(self) -> str:
        """ëœë¤ User-Agent ì„ íƒ"""
        agents = [
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]
        return random.choice(agents)

    def _random_delay(self, min_sec: float = 2.0, max_sec: float = 5.0):
        """ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê¸° ìœ„í•œ ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_sec, max_sec)
        time.sleep(delay)

    def _make_request(self, url: str, params: dict = None) -> Optional[requests.Response]:
        """ì•ˆì „í•œ HTTP ìš”ì²­"""
        try:
            self._random_delay()
            self.headers["User-Agent"] = self._get_random_user_agent()

            response = requests.get(
                url,
                headers=self.headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()
            return response

        except requests.exceptions.RequestException as e:
            print(f"âŒ [{self.name}] ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None

    @abstractmethod
    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """í˜¸í…” ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ê° í”Œë«í¼ë³„ êµ¬í˜„)"""
        pass

    def _normalize_hotel(self, raw_data: dict) -> Dict:
        """í˜¸í…” ë°ì´í„° ì •ê·œí™”"""
        return {
            "name": raw_data.get("name", ""),
            "name_en": raw_data.get("name_en", raw_data.get("name", "")),
            "price_krw": raw_data.get("price_krw", 0),
            "price_usd": raw_data.get("price_usd", int(raw_data.get("price_krw", 0) / 1350)),
            "rating": raw_data.get("rating", 0),
            "review_count": raw_data.get("review_count", 0),
            "address": raw_data.get("address", ""),
            "latitude": raw_data.get("latitude", 0),
            "longitude": raw_data.get("longitude", 0),
            "image_url": raw_data.get("image_url", ""),
            "rooms_left": raw_data.get("rooms_left", -1),
            "booking_url": raw_data.get("booking_url", ""),
            "platform": self.name,
            "scraped_at": datetime.now().isoformat(),
        }


class AgodaScraper(BaseScraper):
    """Agoda ìŠ¤í¬ë˜í¼ - ê¸€ë¡œë²Œ OTA"""

    def __init__(self):
        super().__init__()
        self.name = "Agoda"
        self.name_kr = "ì•„ê³ ë‹¤"
        self.base_url = "https://www.agoda.com"
        self.city_id = 14690  # ê³ ì–‘ì‹œ

    def _build_url(self, checkin: datetime, checkout: datetime) -> str:
        """ê²€ìƒ‰ URL ìƒì„±"""
        params = {
            "city": self.city_id,
            "checkIn": checkin.strftime("%Y-%m-%d"),
            "checkOut": checkout.strftime("%Y-%m-%d"),
            "rooms": 1,
            "adults": 2,
            "children": 0,
            "los": (checkout - checkin).days,
            "sort": "priceLowToHigh",
        }
        return f"{self.base_url}/search?{urlencode(params)}"

    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """Agoda í˜¸í…” ìŠ¤í¬ë˜í•‘"""
        url = self._build_url(checkin, checkout)
        print(f"ğŸ” [{self.name}] ê²€ìƒ‰ ì¤‘...")

        response = self._make_request(url)
        if not response:
            return []

        hotels = self._parse_response(response.text)
        print(f"âœ… [{self.name}] {len(hotels)}ê°œ í˜¸í…” ìˆ˜ì§‘")
        return hotels

    def _parse_response(self, html: str) -> List[Dict]:
        """ì‘ë‹µ íŒŒì‹±"""
        hotels = []

        if not BeautifulSoup:
            return hotels

        soup = BeautifulSoup(html, 'html.parser')

        # __NEXT_DATA__ íŒŒì‹± ì‹œë„
        script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
        if script_tag:
            try:
                data = json.loads(script_tag.string)
                props = data.get('props', {}).get('pageProps', {})
                search_result = props.get('searchResult', {}) or props.get('initialSearchResult', {})
                properties = search_result.get('properties', []) or search_result.get('results', [])

                for prop in properties[:30]:
                    hotel = self._normalize_hotel({
                        "name": prop.get('propertyName', ''),
                        "name_en": prop.get('propertyNameEn', prop.get('propertyName', '')),
                        "price_krw": self._extract_price_krw(prop),
                        "rating": prop.get('rating', 0),
                        "review_count": prop.get('reviewCount', 0),
                        "address": prop.get('address', ''),
                        "latitude": prop.get('latitude', 0),
                        "longitude": prop.get('longitude', 0),
                        "image_url": self._extract_image(prop),
                        "rooms_left": prop.get('roomsLeft', -1),
                        "booking_url": f"{self.base_url}/hotel_{prop.get('propertyId', '')}.html",
                    })
                    if hotel["name"]:
                        hotels.append(hotel)
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âš ï¸ [{self.name}] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

        return hotels

    def _extract_price_krw(self, prop: dict) -> int:
        """ê°€ê²© ì¶”ì¶œ (KRW)"""
        for field in ['price', 'displayPrice', 'pricePerNight']:
            if field in prop:
                val = prop[field]
                if isinstance(val, (int, float)):
                    return int(val * 1350) if val < 1000 else int(val)
                if isinstance(val, dict):
                    return int(val.get('value', 0))
        return 0

    def _extract_image(self, prop: dict) -> str:
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        for field in ['imageUrl', 'heroImage', 'thumbnail']:
            if field in prop:
                img = prop[field]
                if isinstance(img, str):
                    return img
                if isinstance(img, list) and img:
                    return img[0] if isinstance(img[0], str) else img[0].get('url', '')
        return ""


class NaverHotelScraper(BaseScraper):
    """ë„¤ì´ë²„ í˜¸í…” ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        super().__init__()
        self.name = "NaverHotel"
        self.name_kr = "ë„¤ì´ë²„ í˜¸í…”"
        self.base_url = "https://hotel.naver.com"
        self.api_url = "https://hotel.naver.com/api"

    def _build_url(self, checkin: datetime, checkout: datetime) -> str:
        """API URL ìƒì„±"""
        params = {
            "keyword": "ê³ ì–‘",
            "checkIn": checkin.strftime("%Y-%m-%d"),
            "checkOut": checkout.strftime("%Y-%m-%d"),
            "rooms": 1,
            "adults": 2,
            "children": 0,
        }
        return f"{self.api_url}/search/v2?{urlencode(params)}"

    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """ë„¤ì´ë²„ í˜¸í…” ìŠ¤í¬ë˜í•‘"""
        # ë„¤ì´ë²„ëŠ” ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        search_url = f"{self.base_url}/domestic/search?keyword={quote('ê³ ì–‘')}&checkIn={checkin.strftime('%Y-%m-%d')}&checkOut={checkout.strftime('%Y-%m-%d')}"

        print(f"ğŸ” [{self.name}] ê²€ìƒ‰ ì¤‘...")

        response = self._make_request(search_url)
        if not response:
            return []

        hotels = self._parse_response(response.text)
        print(f"âœ… [{self.name}] {len(hotels)}ê°œ í˜¸í…” ìˆ˜ì§‘")
        return hotels

    def _parse_response(self, html: str) -> List[Dict]:
        """ì‘ë‹µ íŒŒì‹±"""
        hotels = []

        if not BeautifulSoup:
            return hotels

        soup = BeautifulSoup(html, 'html.parser')

        # ë„¤ì´ë²„ í˜¸í…”ì€ __NEXT_DATA__ ë˜ëŠ” window.__PRELOADED_STATE__ ì‚¬ìš©
        script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
        if script_tag:
            try:
                data = json.loads(script_tag.string)
                # ë„¤ì´ë²„ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
                page_props = data.get('props', {}).get('pageProps', {})
                hotel_list = page_props.get('hotels', []) or page_props.get('searchResult', {}).get('hotels', [])

                for hotel_data in hotel_list[:30]:
                    hotel = self._normalize_hotel({
                        "name": hotel_data.get('name', ''),
                        "price_krw": hotel_data.get('lowestPrice', hotel_data.get('price', 0)),
                        "rating": hotel_data.get('rating', hotel_data.get('score', 0)),
                        "review_count": hotel_data.get('reviewCount', 0),
                        "address": hotel_data.get('address', ''),
                        "latitude": hotel_data.get('latitude', hotel_data.get('y', 0)),
                        "longitude": hotel_data.get('longitude', hotel_data.get('x', 0)),
                        "image_url": hotel_data.get('imageUrl', hotel_data.get('thumbnailUrl', '')),
                        "booking_url": f"{self.base_url}/hotels/{hotel_data.get('id', '')}",
                    })
                    if hotel["name"]:
                        hotels.append(hotel)
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âš ï¸ [{self.name}] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

        # HTML ìš”ì†Œ í´ë°±
        if not hotels:
            hotel_cards = soup.find_all('div', class_=re.compile(r'hotel|item|card', re.I))
            for card in hotel_cards[:20]:
                name_el = card.find(['h2', 'h3', 'span'], class_=re.compile(r'name|title', re.I))
                price_el = card.find(['span', 'div'], class_=re.compile(r'price', re.I))

                if name_el:
                    name = name_el.get_text(strip=True)
                    price = 0
                    if price_el:
                        price_text = price_el.get_text(strip=True)
                        price_match = re.search(r'[\d,]+', price_text.replace(',', ''))
                        if price_match:
                            price = int(price_match.group())

                    hotel = self._normalize_hotel({
                        "name": name,
                        "price_krw": price,
                    })
                    hotels.append(hotel)

        return hotels


class GoodChoiceScraper(BaseScraper):
    """ì—¬ê¸°ì–´ë•Œ ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        super().__init__()
        self.name = "GoodChoice"
        self.name_kr = "ì—¬ê¸°ì–´ë•Œ"
        self.base_url = "https://www.goodchoice.kr"

    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """ì—¬ê¸°ì–´ë•Œ ìŠ¤í¬ë˜í•‘"""
        # ì—¬ê¸°ì–´ë•Œ ëª¨ë°”ì¼ ì›¹ ê²€ìƒ‰
        search_url = f"{self.base_url}/search?keyword={quote('ê³ ì–‘')}&check_in={checkin.strftime('%Y-%m-%d')}&check_out={checkout.strftime('%Y-%m-%d')}"

        print(f"ğŸ” [{self.name}] ê²€ìƒ‰ ì¤‘...")

        # ëª¨ë°”ì¼ User-Agentë¡œ ë³€ê²½ (ë” ê°„ë‹¨í•œ HTML)
        self.headers["User-Agent"] = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"

        response = self._make_request(search_url)
        if not response:
            return []

        hotels = self._parse_response(response.text)
        print(f"âœ… [{self.name}] {len(hotels)}ê°œ í˜¸í…” ìˆ˜ì§‘")
        return hotels

    def _parse_response(self, html: str) -> List[Dict]:
        """ì‘ë‹µ íŒŒì‹±"""
        hotels = []

        if not BeautifulSoup:
            return hotels

        soup = BeautifulSoup(html, 'html.parser')

        # ì—¬ê¸°ì–´ë•Œ ì¹´ë“œ ìš”ì†Œ ì°¾ê¸°
        hotel_items = soup.find_all('li', class_=re.compile(r'list_item|hotel_item', re.I))
        if not hotel_items:
            hotel_items = soup.find_all('div', class_=re.compile(r'accommodation|hotel|room', re.I))

        for item in hotel_items[:30]:
            try:
                # ì´ë¦„
                name_el = item.find(['h3', 'strong', 'a'], class_=re.compile(r'name|title', re.I))
                name = name_el.get_text(strip=True) if name_el else ""

                # ê°€ê²©
                price = 0
                price_el = item.find(['span', 'strong'], class_=re.compile(r'price|won', re.I))
                if price_el:
                    price_text = price_el.get_text(strip=True)
                    price_match = re.search(r'[\d,]+', price_text.replace(',', ''))
                    if price_match:
                        price = int(price_match.group())

                # í‰ì 
                rating = 0
                rating_el = item.find(['span', 'em'], class_=re.compile(r'score|rating', re.I))
                if rating_el:
                    rating_text = rating_el.get_text(strip=True)
                    rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                    if rating_match:
                        rating = float(rating_match.group(1))

                # ì´ë¯¸ì§€
                img_el = item.find('img')
                image_url = ""
                if img_el:
                    image_url = img_el.get('src') or img_el.get('data-src', '')

                if name:
                    hotel = self._normalize_hotel({
                        "name": name,
                        "price_krw": price,
                        "rating": rating,
                        "image_url": image_url,
                    })
                    hotels.append(hotel)

            except Exception as e:
                continue

        return hotels


class YanoljaScraper(BaseScraper):
    """ì•¼ë†€ì ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        super().__init__()
        self.name = "Yanolja"
        self.name_kr = "ì•¼ë†€ì"
        self.base_url = "https://www.yanolja.com"

    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """ì•¼ë†€ì ìŠ¤í¬ë˜í•‘"""
        # ì•¼ë†€ì ê²€ìƒ‰ (ê³ ì–‘ì‹œ ì§€ì—­ì½”ë“œ ì‚¬ìš©)
        search_url = f"{self.base_url}/search/{quote('ê³ ì–‘')}?checkin={checkin.strftime('%Y-%m-%d')}&checkout={checkout.strftime('%Y-%m-%d')}"

        print(f"ğŸ” [{self.name}] ê²€ìƒ‰ ì¤‘...")

        response = self._make_request(search_url)
        if not response:
            return []

        hotels = self._parse_response(response.text)
        print(f"âœ… [{self.name}] {len(hotels)}ê°œ í˜¸í…” ìˆ˜ì§‘")
        return hotels

    def _parse_response(self, html: str) -> List[Dict]:
        """ì‘ë‹µ íŒŒì‹±"""
        hotels = []

        if not BeautifulSoup:
            return hotels

        soup = BeautifulSoup(html, 'html.parser')

        # __NEXT_DATA__ ì‹œë„ (Next.js ì•±)
        script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
        if script_tag:
            try:
                data = json.loads(script_tag.string)
                page_props = data.get('props', {}).get('pageProps', {})

                # ë‹¤ì–‘í•œ í‚¤ ì‹œë„
                for key in ['hotels', 'accommodations', 'places', 'searchResults', 'items']:
                    items = page_props.get(key, [])
                    if not items and 'data' in page_props:
                        items = page_props['data'].get(key, [])

                    if items:
                        for item in items[:30]:
                            hotel = self._normalize_hotel({
                                "name": item.get('name', item.get('placeName', '')),
                                "price_krw": item.get('price', item.get('minPrice', item.get('lowestPrice', 0))),
                                "rating": item.get('rating', item.get('reviewScore', 0)),
                                "review_count": item.get('reviewCount', 0),
                                "address": item.get('address', ''),
                                "latitude": item.get('latitude', item.get('y', 0)),
                                "longitude": item.get('longitude', item.get('x', 0)),
                                "image_url": item.get('imageUrl', item.get('thumbnailUrl', '')),
                                "booking_url": f"{self.base_url}/place/{item.get('id', '')}",
                            })
                            if hotel["name"]:
                                hotels.append(hotel)
                        break
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âš ï¸ [{self.name}] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

        # HTML í´ë°±
        if not hotels:
            cards = soup.find_all(['div', 'article'], class_=re.compile(r'PlaceCard|accommodation|hotel', re.I))
            for card in cards[:20]:
                name_el = card.find(['h2', 'h3', 'a'], class_=re.compile(r'name|title', re.I))
                if name_el:
                    hotel = self._normalize_hotel({
                        "name": name_el.get_text(strip=True),
                    })
                    hotels.append(hotel)

        return hotels


class CoupangTravelScraper(BaseScraper):
    """ì¿ íŒ¡ íŠ¸ë˜ë¸” ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        super().__init__()
        self.name = "CoupangTravel"
        self.name_kr = "ì¿ íŒ¡ íŠ¸ë˜ë¸”"
        self.base_url = "https://travel.coupang.com"

    def scrape(self, checkin: datetime, checkout: datetime) -> List[Dict]:
        """ì¿ íŒ¡ íŠ¸ë˜ë¸” ìŠ¤í¬ë˜í•‘"""
        # ì¿ íŒ¡ íŠ¸ë˜ë¸” ê²€ìƒ‰
        search_url = f"{self.base_url}/np/search/stays?q={quote('ê³ ì–‘')}&checkin={checkin.strftime('%Y-%m-%d')}&checkout={checkout.strftime('%Y-%m-%d')}"

        print(f"ğŸ” [{self.name}] ê²€ìƒ‰ ì¤‘...")

        response = self._make_request(search_url)
        if not response:
            return []

        hotels = self._parse_response(response.text)
        print(f"âœ… [{self.name}] {len(hotels)}ê°œ í˜¸í…” ìˆ˜ì§‘")
        return hotels

    def _parse_response(self, html: str) -> List[Dict]:
        """ì‘ë‹µ íŒŒì‹±"""
        hotels = []

        if not BeautifulSoup:
            return hotels

        soup = BeautifulSoup(html, 'html.parser')

        # JSON ë°ì´í„° ì¶”ì¶œ ì‹œë„
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and ('__PRELOADED_STATE__' in script.string or 'window.__data' in script.string):
                try:
                    # JSON ì¶”ì¶œ
                    match = re.search(r'=\s*(\{.*?\});', script.string, re.DOTALL)
                    if match:
                        data = json.loads(match.group(1))

                        # í˜¸í…” ë°ì´í„° ì°¾ê¸°
                        for key in ['stays', 'hotels', 'accommodations', 'results']:
                            if key in data:
                                for item in data[key][:30]:
                                    hotel = self._normalize_hotel({
                                        "name": item.get('name', ''),
                                        "price_krw": item.get('price', item.get('salePrice', 0)),
                                        "rating": item.get('rating', 0),
                                        "image_url": item.get('imageUrl', ''),
                                        "booking_url": f"{self.base_url}/np/stays/{item.get('id', '')}",
                                    })
                                    if hotel["name"]:
                                        hotels.append(hotel)
                                break
                except:
                    continue

        # HTML í´ë°±
        if not hotels:
            cards = soup.find_all('div', class_=re.compile(r'stay|hotel|item', re.I))
            for card in cards[:20]:
                name_el = card.find(['h2', 'h3', 'span'], class_=re.compile(r'name|title', re.I))
                if name_el:
                    hotel = self._normalize_hotel({
                        "name": name_el.get_text(strip=True),
                    })
                    hotels.append(hotel)

        return hotels


class KoreanOTAScraper:
    """
    ë‹¤ì¤‘ í”Œë«í¼ í•œêµ­ OTA ìŠ¤í¬ë˜í¼ ë§¤ë‹ˆì €

    5ê°œ í”Œë«í¼ì—ì„œ ë¶„ì‚° ìˆ˜ì§‘í•˜ì—¬:
    1. ë¦¬ìŠ¤í¬ ë¶„ì‚° (ê° í”Œë«í¼ë‹¹ ì†ŒëŸ‰ ìš”ì²­)
    2. ì •ë³´ í’ë¶€í™” (ê°€ê²© ë¹„êµ, ì¬ê³  êµì°¨ í™•ì¸)
    3. ì‹ ë¢°ì„± í–¥ìƒ (ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ê²€ì¦)
    """

    def __init__(self):
        self.target_areas = ["Goyang", "Ilsan", "Paju", "Hongdae", "Sangam"]

        # ì§€ì› í”Œë«í¼
        self.platforms = {
            'agoda': {'scraper': AgodaScraper(), 'weight': 2},  # ê¸€ë¡œë²Œ, ì˜ì–´ ë°ì´í„°
            'naver': {'scraper': NaverHotelScraper(), 'weight': 2},  # êµ­ë‚´ ìµœëŒ€
            'goodchoice': {'scraper': GoodChoiceScraper(), 'weight': 1},
            'yanolja': {'scraper': YanoljaScraper(), 'weight': 1},
            'coupang': {'scraper': CoupangTravelScraper(), 'weight': 1},
        }

        # ì½˜ì„œíŠ¸ ë‚ ì§œ (D-Day)
        self.concert_date = datetime(2026, 6, 12)

    def scrape_all(self, checkin: datetime = None, checkout: datetime = None,
                   platforms: List[str] = None, max_per_platform: int = 2) -> Dict:
        """
        ëª¨ë“  í”Œë«í¼ì—ì„œ í˜¸í…” ì •ë³´ ìˆ˜ì§‘

        Args:
            checkin: ì²´í¬ì¸ ë‚ ì§œ (ê¸°ë³¸: ì½˜ì„œíŠ¸ ì „ë‚ )
            checkout: ì²´í¬ì•„ì›ƒ ë‚ ì§œ (ê¸°ë³¸: ì½˜ì„œíŠ¸ ë‹¤ìŒë‚ )
            platforms: ì‚¬ìš©í•  í”Œë«í¼ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ì „ì²´)
            max_per_platform: í”Œë«í¼ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜

        Returns:
            í”Œë«í¼ë³„ í˜¸í…” ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        if not checkin:
            checkin = self.concert_date - timedelta(days=1)
        if not checkout:
            checkout = self.concert_date + timedelta(days=1)

        if not platforms:
            platforms = list(self.platforms.keys())

        print("=" * 60)
        print(f"ğŸš€ ARMY Stay Hub - ë‹¤ì¤‘ í”Œë«í¼ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print(f"ğŸ“… ì²´í¬ì¸: {checkin.strftime('%Y-%m-%d')}")
        print(f"ğŸ“… ì²´í¬ì•„ì›ƒ: {checkout.strftime('%Y-%m-%d')}")
        print(f"ğŸ¯ í”Œë«í¼: {', '.join(platforms)}")
        print("=" * 60)

        results = {
            "meta": {
                "checkin": checkin.isoformat(),
                "checkout": checkout.isoformat(),
                "scraped_at": datetime.now().isoformat(),
                "platforms_used": platforms,
            },
            "by_platform": {},
            "all_hotels": [],
        }

        # í”Œë«í¼ë³„ ìŠ¤í¬ë˜í•‘ (ìˆœì°¨ì , ë”œë ˆì´ í¬í•¨)
        for platform_name in platforms:
            if platform_name not in self.platforms:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í”Œë«í¼: {platform_name}")
                continue

            platform = self.platforms[platform_name]
            scraper = platform['scraper']

            try:
                hotels = scraper.scrape(checkin, checkout)
                results["by_platform"][platform_name] = {
                    "name_kr": scraper.name_kr,
                    "count": len(hotels),
                    "hotels": hotels,
                }
                results["all_hotels"].extend(hotels)

                # í”Œë«í¼ ê°„ ë”œë ˆì´ (5~10ì´ˆ)
                if platform_name != platforms[-1]:
                    delay = random.uniform(5, 10)
                    print(f"â³ ë‹¤ìŒ í”Œë«í¼ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)

            except Exception as e:
                print(f"âŒ [{platform_name}] ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                results["by_platform"][platform_name] = {
                    "name_kr": scraper.name_kr,
                    "count": 0,
                    "hotels": [],
                    "error": str(e),
                }

        # í†µê³„
        total = len(results["all_hotels"])
        print("\n" + "=" * 60)
        print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ! ì´ {total}ê°œ í˜¸í…” ìˆ˜ì§‘")
        for p_name, p_data in results["by_platform"].items():
            print(f"   - {p_data['name_kr']}: {p_data['count']}ê°œ")
        print("=" * 60)

        return results

    def scrape_distributed(self, checkin: datetime = None, checkout: datetime = None) -> List[Dict]:
        """
        í•˜ë£¨ 10íšŒ ë¶„ì‚° ìŠ¤í¬ë˜í•‘ìš© - ë§¤ ì‹¤í–‰ë§ˆë‹¤ ë‹¤ë¥¸ í”Œë«í¼ ì„ íƒ

        ë¶„ì‚° ì „ëµ:
        - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëœë¤ ì„ íƒ (agoda, naverëŠ” 2ë°° í™•ë¥ )
        - í•œ ë²ˆì— 1~2ê°œ í”Œë«í¼ë§Œ ì‚¬ìš©
        - ìš”ì²­ ê°„ ì¶©ë¶„í•œ ë”œë ˆì´
        """
        if not checkin:
            checkin = self.concert_date - timedelta(days=1)
        if not checkout:
            checkout = self.concert_date + timedelta(days=1)

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í”Œë«í¼ ì„ íƒ
        weighted_platforms = []
        for name, config in self.platforms.items():
            weighted_platforms.extend([name] * config['weight'])

        # 1~2ê°œ í”Œë«í¼ ëœë¤ ì„ íƒ
        num_platforms = random.randint(1, 2)
        selected = random.sample(weighted_platforms, min(num_platforms, len(set(weighted_platforms))))
        selected = list(set(selected))  # ì¤‘ë³µ ì œê±°

        print(f"ğŸ² ì´ë²ˆ ì‹¤í–‰ í”Œë«í¼: {', '.join(selected)}")

        # ì„ íƒëœ í”Œë«í¼ë§Œ ìŠ¤í¬ë˜í•‘
        results = self.scrape_all(checkin, checkout, platforms=selected)

        return results["all_hotels"]

    def merge_with_simulation(self, scraped_hotels: List[Dict], simulated_hotels: List[Dict]) -> List[Dict]:
        """
        ìŠ¤í¬ë˜í•‘ ë°ì´í„°ì™€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë³‘í•©

        ìŠ¤í¬ë˜í•‘ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ í˜¸í…”ì˜ ê°€ê²©/ì¬ê³  ì—…ë°ì´íŠ¸
        ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìœ ì§€
        """
        # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­
        scraped_by_name = {h['name'].lower(): h for h in scraped_hotels if h.get('name')}
        scraped_by_name_en = {h.get('name_en', '').lower(): h for h in scraped_hotels if h.get('name_en')}

        merged = []
        updated_count = 0

        for sim_hotel in simulated_hotels:
            hotel = sim_hotel.copy()

            # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
            sim_name = sim_hotel.get('name', '').lower()
            sim_name_en = sim_hotel.get('name_en', '').lower()

            matched = scraped_by_name.get(sim_name) or scraped_by_name_en.get(sim_name_en)

            if matched:
                # ì‹¤ì œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
                if matched.get('price_krw') and matched['price_krw'] > 0:
                    hotel['price_krw'] = matched['price_krw']
                    hotel['price_usd'] = matched.get('price_usd', int(matched['price_krw'] / 1350))

                if matched.get('rating') and matched['rating'] > 0:
                    hotel['rating'] = matched['rating']

                if matched.get('rooms_left') and matched['rooms_left'] >= 0:
                    hotel['rooms_left'] = matched['rooms_left']
                    hotel['status'] = "ì˜ˆì•½ ë§ˆê°" if matched['rooms_left'] == 0 else f"ë‚¨ì€ ê°ì‹¤ {matched['rooms_left']}ê°œ"
                    hotel['status_en'] = "Sold Out" if matched['rooms_left'] == 0 else f"{matched['rooms_left']} rooms left"
                    hotel['is_available'] = matched['rooms_left'] > 0

                hotel['_data_source'] = matched.get('platform', 'scraped')
                updated_count += 1
            else:
                hotel['_data_source'] = 'simulation'

            hotel['last_update'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            merged.append(hotel)

        print(f"ğŸ“Š ë°ì´í„° ë³‘í•© ì™„ë£Œ: {updated_count}/{len(simulated_hotels)}ê°œ ì‹¤ì œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸")

        return merged


def test_multi_platform():
    """ë‹¤ì¤‘ í”Œë«í¼ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë‹¤ì¤‘ í”Œë«í¼ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    scraper = KoreanOTAScraper()

    # í…ŒìŠ¤íŠ¸ ë‚ ì§œ
    checkin = datetime(2026, 6, 11)
    checkout = datetime(2026, 6, 13)

    # ë¶„ì‚° ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (1-2ê°œ í”Œë«í¼ë§Œ)
    hotels = scraper.scrape_distributed(checkin, checkout)

    if hotels:
        print(f"\nğŸ“Š ìˆ˜ì§‘ëœ í˜¸í…” ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):")
        for i, hotel in enumerate(hotels[:5], 1):
            print(f"\n{i}. {hotel['name']}")
            print(f"   í”Œë«í¼: {hotel.get('platform', 'unknown')}")
            print(f"   ê°€ê²©: â‚©{hotel.get('price_krw', 0):,}")
            print(f"   í‰ì : {hotel.get('rating', 0)}")
    else:
        print("\nâš ï¸ í˜¸í…” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("   JavaScript ë Œë”ë§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   â†’ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")

    return hotels


if __name__ == "__main__":
    test_multi_platform()
